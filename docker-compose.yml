version: "3.8"

services:
  open-webui:
    container_name: open-webui
    hostname: open-webui
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - 8080
    volumes:
      - /mnt/user/flash_storage/ollama/data:/app/backend/data
      - /mnt/user/flash_storage/ollama/root:/root/.ollama
    environment:
      - OLLAMA_BASE_URL
      - CUSTOM_NAME
      - OPENAI_API_BASE_URLS
      -


    networks:
      - ollama
      - cloud-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]


networks:
  ollama:
    external: true

  cloud-net:
    external: true


docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://chaty.bentleyhensel.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main


docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main.


kNjPWGb2DLQg4ccuzl0J2Wir3uXBZgbhA


https://litellm.vercel.app/docs/proxy/configs#quick-start


https://docs.openwebui.com/getting-started/#quick-start-with-docker-

